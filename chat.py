from langchain.prompts import ChatPromptTemplate
# from langchain_community.llms.ollama import Ollama # Ollamaã‚’å‰Šé™¤
from langchain_openai import ChatOpenAI # ChatOpenAI ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Prompt Template (å¤‰æ›´ãªã—)
PROMPT_TEMPLATE = """
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€ã†ã¾ãæ´»ç”¨ã•ã›ãªãŒã‚‰è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š

{context}

---

ä¸Šè¨˜ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€æ¬¡ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š{question}
"""

def query_rag(query_text: str) -> dict:
    """
    RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã€æŒ‡å®šã•ã‚ŒãŸè³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã¨ã‚½ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚
    LLMã¨ã—ã¦GPT-4oã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    """
    try:
        # 1. Embeddingsã¨Vectorstoreã®ãƒ­ãƒ¼ãƒ‰ (å¤‰æ›´ãªã—)
        embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        # vectorstoreãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if not os.path.exists("vectorstore"):
             raise FileNotFoundError("The 'vectorstore' directory does not exist. Please ensure it's created and populated.")
        db = FAISS.load_local("vectorstore", embedding, allow_dangerous_deserialization=True)

        # 2. Retrieverã®æº–å‚™ã¨é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å–å¾— (å¤‰æ›´ãªã—)
        retriever = db.as_retriever()
        docs = retriever.get_relevant_documents(query_text)
        if not docs:
            print(f"Warning: No relevant documents found for query: '{query_text}'")
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—ã§ç¶šè¡Œã™ã‚‹ã‹ã€ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ã‹ã‚’é¸æŠã§ãã¾ã™
            # ã“ã“ã§ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—ã§ç¶šè¡Œã—ã¾ã™
            context_text = "åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        else:
            # å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ (ä¸Šä½5ä»¶ã‚’ä½¿ç”¨)
            context_text = "\n\n---\n\n".join([doc.page_content for doc in docs[:5]])

        # 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™ (å¤‰æ›´ãªã—)
        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
        prompt = prompt_template.format(context=context_text, question=query_text)

        # 4. LLMã®åˆæœŸåŒ– (Ollama -> ChatOpenAI with gpt-4o)
        # model = Ollama(model="gemma3:12b") # å¤ã„Ollamaãƒ¢ãƒ‡ãƒ«
        model = ChatOpenAI(
            openai_api_key=OPENAI_API_KEY,
            model="gpt-4o", # ãƒ¢ãƒ‡ãƒ«ã‚’gpt-4oã«å¤‰æ›´
            temperature=0.7 # å¿…è¦ã«å¿œã˜ã¦æ¸©åº¦ã‚’è¨­å®š (0ã¯æ±ºå®šè«–çš„ãªå‡ºåŠ›)
        )

        # 5. LLMã®å®Ÿè¡Œã¨å¿œç­”ã®å–å¾—
        response = model.invoke(prompt)

        # 6. ã‚½ãƒ¼ã‚¹ã®æº–å‚™ (å¤‰æ›´ãªã—ã€ãŸã ã—docsãŒç©ºã®å ´åˆã‚’è€ƒæ…®)
        sources = []
        if docs:
            sources = [{"title": doc.metadata.get('title', 'N/A'), "url": doc.metadata.get('source', 'N/A')} for doc in docs[:5]]
            
        # ğŸ”¥ printãƒ­ã‚°ï¼ˆCloud Runãƒ­ã‚°ã«å‡ºã‚‹ï¼‰
        print("ğŸ§‘â€ğŸ’¬ User query:", query_text)
        print("ğŸ¤– AI answer:", response.content)
        print("ğŸ“š Sources:", sources)

        # 7. çµæœã®è¿”å´ (response.contentã‚’ä½¿ç”¨)
        return {
            # "answer": response, # Ollamaã®å¿œç­”ã¯é€šå¸¸æ–‡å­—åˆ—ã ãŒã€ChatModelã¯AIMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            "answer": response.content, # AIMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’å–å¾—
            "sources": sources
        }

    except FileNotFoundError as e:
        print(f"Error: {e}")
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«è¿”ã™æƒ…å ±ï¼ˆä¾‹ï¼‰
        return {"answer": "ã‚¨ãƒ©ãƒ¼: ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", "sources": []}
    except Exception as e:
        print(f"An unexpected error occurred in query_rag: {e}")
        # ãã®ä»–ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«è¿”ã™æƒ…å ±ï¼ˆä¾‹ï¼‰
        return {"answer": f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", "sources": []}